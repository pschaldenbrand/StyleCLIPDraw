{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Style-ClipDraw-1.0 Refactored",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pschaldenbrand/StyleCLIPDraw/blob/master/StyleCLIPDraw.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7DCzf-EHzL_"
      },
      "source": [
        "**STEPS:**\n",
        "\n",
        "\n",
        "1. Click \"Connect\" in the top right corner\n",
        "2. Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "2. Click the run button on \"Pre Installation\". This will install dependencies, it may take a while.\n",
        "2. **Important:** Runtime -> Restart Runtime\n",
        "3. Run the \"Imports and Notebook Utilities\" and \"Load CLIP\" sections.\n",
        "\n",
        "\n",
        "5. \"Curve Optimizer\" will synthesize a drawing to match your text. You can edit the text prompt at the top of the code block.\n",
        "6. \"Video Renderer\" can create videos that show the optimization process, and videos that render a drawing stroke-by-stroke.\n",
        "\n",
        "**PARAMETERS:**\n",
        "\n",
        "\n",
        "1. Change the \"style_root\" and \"prompt\" in the **Curve Optimizier** section.\n",
        "2. Change the ARGUMENTS of curves in the **Curve Optimizier** section, especially num_paths.\n",
        "3. Change the \"opt_iter\" to control style transfer in one iteration in **Optimizer and learning rate** section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dyyH781qzIC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "dbdf01cf-31e4-4a02-b25b-3f200d8bd06f"
      },
      "source": [
        "#@title Pre Installation {vertical-output: true}\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "# !pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
        "%cd /content/\n",
        "!pip install svgwrite\n",
        "!pip install svgpathtools\n",
        "!pip install cssutils\n",
        "!pip install numba\n",
        "!pip install torch-tools\n",
        "!pip install visdom\n",
        "\n",
        "!git clone https://github.com/BachiLi/diffvg\n",
        "%cd diffvg\n",
        "# !ls\n",
        "!git submodule update --init --recursive\n",
        "!python setup.py install\n",
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git --no-deps"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA version: 11.1\n",
            "/content\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.1-py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 4.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite\n",
            "Successfully installed svgwrite-1.4.1\n",
            "Collecting svgpathtools\n",
            "  Downloading svgpathtools-1.4.2-py2.py3-none-any.whl (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from svgpathtools) (1.19.5)\n",
            "Requirement already satisfied: svgwrite in /usr/local/lib/python3.7/dist-packages (from svgpathtools) (1.4.1)\n",
            "Installing collected packages: svgpathtools\n",
            "Successfully installed svgpathtools-1.4.2\n",
            "Collecting cssutils\n",
            "  Downloading cssutils-2.3.0-py3-none-any.whl (404 kB)\n",
            "\u001b[K     |████████████████████████████████| 404 kB 8.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from cssutils) (4.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cssutils) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->cssutils) (3.5.0)\n",
            "Installing collected packages: cssutils\n",
            "Successfully installed cssutils-2.3.0\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (0.51.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from numba) (1.19.5)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba) (57.4.0)\n",
            "Collecting torch-tools\n",
            "  Downloading torch_tools-0.1.5-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-tools) (1.19.5)\n",
            "Collecting pyaml\n",
            "  Downloading pyaml-21.8.3-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torch-tools) (1.9.0+cu102)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-tools) (2.11.3)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.7/dist-packages (from torch-tools) (1.4.23)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from torch-tools) (2.4.1)\n",
            "Collecting imageio-ffmpeg\n",
            "  Downloading imageio_ffmpeg-0.4.5-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.9 MB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from torch-tools) (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-tools) (4.62.2)\n",
            "Collecting visdom\n",
            "  Downloading visdom-0.1.8.9.tar.gz (676 kB)\n",
            "\u001b[K     |████████████████████████████████| 676 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from torch-tools) (0.10.0+cu102)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->torch-tools) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-tools) (2.0.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml->torch-tools) (3.13)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->torch-tools) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->torch-tools) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn->torch-tools) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->torch-tools) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->torch-tools) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->torch-tools) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->torch-tools) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib>=2.2->seaborn->torch-tools) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->torch-tools) (2018.9)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->torch-tools) (1.1.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy->torch-tools) (4.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->torch-tools) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy->torch-tools) (3.7.4.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom->torch-tools) (2.23.0)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom->torch-tools) (5.1.1)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom->torch-tools) (22.2.1)\n",
            "Collecting jsonpatch\n",
            "  Downloading jsonpatch-1.32-py2.py3-none-any.whl (12 kB)\n",
            "Collecting torchfile\n",
            "  Downloading torchfile-0.1.0.tar.gz (5.2 kB)\n",
            "Collecting websocket-client\n",
            "  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s \n",
            "\u001b[?25hCollecting jsonpointer>=1.9\n",
            "  Downloading jsonpointer-2.1-py2.py3-none-any.whl (7.4 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torch-tools) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torch-tools) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torch-tools) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom->torch-tools) (3.0.4)\n",
            "Building wheels for collected packages: visdom, torchfile\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.1.8.9-py3-none-any.whl size=655250 sha256=8307480f1418bf62505705e5b92ad7bb3bc35bf134ae0ff9ca0d218192f294f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/d1/9b/cde923274eac9cbb6ff0d8c7c72fe30a3da9095a38fd50bbf1\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-py3-none-any.whl size=5710 sha256=7e662589f93025fabb1e8724e9102b6019203547bedf162edce5588eded29277\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/5c/3a/a80e1c65880945c71fd833408cd1e9a8cb7e2f8f37620bb75b\n",
            "Successfully built visdom torchfile\n",
            "Installing collected packages: jsonpointer, websocket-client, torchfile, jsonpatch, humanfriendly, visdom, pyaml, imageio-ffmpeg, coloredlogs, torch-tools\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 imageio-ffmpeg-0.4.5 jsonpatch-1.32 jsonpointer-2.1 pyaml-21.8.3 torch-tools-0.1.5 torchfile-0.1.0 visdom-0.1.8.9 websocket-client-1.2.1\n",
            "Requirement already satisfied: visdom in /usr/local/lib/python3.7/dist-packages (0.1.8.9)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.7/dist-packages (from visdom) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from visdom) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from visdom) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from visdom) (2.23.0)\n",
            "Requirement already satisfied: torchfile in /usr/local/lib/python3.7/dist-packages (from visdom) (0.1.0)\n",
            "Requirement already satisfied: pyzmq in /usr/local/lib/python3.7/dist-packages (from visdom) (22.2.1)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.7/dist-packages (from visdom) (1.32)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.7/dist-packages (from visdom) (5.1.1)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from visdom) (1.2.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from visdom) (7.1.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.7/dist-packages (from jsonpatch->visdom) (2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->visdom) (2021.5.30)\n",
            "Cloning into 'diffvg'...\n",
            "remote: Enumerating objects: 266, done.\u001b[K\n",
            "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 266 (delta 36), reused 32 (delta 32), pack-reused 216\u001b[K\n",
            "Receiving objects: 100% (266/266), 10.26 MiB | 28.46 MiB/s, done.\n",
            "Resolving deltas: 100% (99/99), done.\n",
            "/content/diffvg\n",
            "Submodule 'pybind11' (https://github.com/pybind/pybind11.git) registered for path 'pybind11'\n",
            "Submodule 'thrust' (https://github.com/thrust/thrust.git) registered for path 'thrust'\n",
            "Cloning into '/content/diffvg/pybind11'...\n",
            "Cloning into '/content/diffvg/thrust'...\n",
            "Submodule path 'pybind11': checked out '72b06b86b3824781f31c790dfce67e26e6307816'\n",
            "Submodule 'tools/clang' (https://github.com/wjakob/clang-cindex-python3.git) registered for path 'pybind11/tools/clang'\n",
            "Cloning into '/content/diffvg/pybind11/tools/clang'...\n",
            "Submodule path 'pybind11/tools/clang': checked out '6a00cbc4a9b8e68b71caf7f774b3f9c753ae84d5'\n",
            "Submodule path 'thrust': checked out 'ff00c813aa3a6bbfd1d8c338313f382b6b340005'\n",
            "Submodule 'cub' (https://github.com/thrust/cub.git) registered for path 'thrust/dependencies/cub'\n",
            "Cloning into '/content/diffvg/thrust/dependencies/cub'...\n",
            "Submodule path 'thrust/dependencies/cub': checked out '2442f44532ffcc53298c7e3a298feb5134563860'\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating diffvg.egg-info\n",
            "writing diffvg.egg-info/PKG-INFO\n",
            "writing dependency_links to diffvg.egg-info/dependency_links.txt\n",
            "writing requirements to diffvg.egg-info/requires.txt\n",
            "writing top-level names to diffvg.egg-info/top_level.txt\n",
            "writing manifest file 'diffvg.egg-info/SOURCES.txt'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'diffvg.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/parse_svg.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/__init__.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/save_svg.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/image.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/color.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/shape.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/pixel_filter.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/device.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/optimize_svg.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "copying pydiffvg/render_pytorch.py -> build/lib.linux-x86_64-3.7/pydiffvg\n",
            "creating build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/render_tensorflow.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/__init__.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/image.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/color.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/shape.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/pixel_filter.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "copying pydiffvg_tensorflow/device.py -> build/lib.linux-x86_64-3.7/pydiffvg_tensorflow\n",
            "running build_ext\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Python: /usr/lib/python3.7/config-3.7m-x86_64-linux-gnu/libpython3.7m.so (found suitable version \"3.7.12\", minimum required is \"3.7\") found components:  Development \n",
            "-- pybind11 v2.6.0 dev\n",
            "-- Performing Test HAS_FLTO\n",
            "-- Performing Test HAS_FLTO - Success\n",
            "-- Using pybind11: (version \"2.6.0\" dev)\n",
            "-- Build with CUDA support\n",
            "-- Looking for pthread.h\n",
            "-- Looking for pthread.h - found\n",
            "-- Looking for pthread_create\n",
            "-- Looking for pthread_create - not found\n",
            "-- Looking for pthread_create in pthreads\n",
            "-- Looking for pthread_create in pthreads - not found\n",
            "-- Looking for pthread_create in pthread\n",
            "-- Looking for pthread_create in pthread - found\n",
            "-- Found Threads: TRUE  \n",
            "-- Found CUDA: /usr/local/cuda (found suitable version \"11.1\", minimum required is \"10\") \n",
            "-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.7m.so (found version \"3.7.12\") \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/diffvg/build/temp.linux-x86_64-3.7\n",
            "[  9%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/diffvg.dir/diffvg_generated_scene.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target diffvg_tf_data_ptr_no_cxx11_abi\u001b[0m\n",
            "[ 18%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/diffvg.dir/diffvg_generated_diffvg.cpp.o\u001b[0m\n",
            "\u001b[35m\u001b[1mScanning dependencies of target diffvg_tf_data_ptr_cxx11_abi\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object pydiffvg_tensorflow/custom_ops/CMakeFiles/diffvg_tf_data_ptr_cxx11_abi.dir/data_ptr.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object pydiffvg_tensorflow/custom_ops/CMakeFiles/diffvg_tf_data_ptr_no_cxx11_abi.dir/data_ptr.cc.o\u001b[0m\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27:0,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:23,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:104:13: warning: Thrust requires C++14. Please pass -std=c++14 to your compiler. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   THRUST_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                            \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36:0,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:32,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:34,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:279,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:105,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:25,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:724,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:58,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.h:79,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/detail/merge.h:22,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/execution_policy.h:51,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:32,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:129:13: warning: CUB requires C++14. Please pass -std=c++14 to your compiler. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   CUB_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                         \n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27:0,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:23,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:104:13: warning: Thrust requires C++14. Please pass -std=c++14 to your compiler. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   THRUST_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                            \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36:0,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:32,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:34,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:279,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:105,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:25,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:724,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:58,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.h:79,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/detail/merge.h:22,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/execution_policy.h:51,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:32,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:129:13: warning: CUB requires C++14. Please pass -std=c++14 to your compiler. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   CUB_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                         \n",
            "/content/diffvg/pybind11/include/pybind11/cast.h(1041): warning: pointless comparison of unsigned integer with zero\n",
            "          detected during:\n",
            "            instantiation of \"__nv_bool pybind11::detail::type_caster<T, pybind11::detail::enable_if_t<<expression>, void>>::load(pybind11::handle, __nv_bool) [with T=pybind11::detail::intrinsic_t<std::size_t>]\" \n",
            "(2004): here\n",
            "            instantiation of \"__nv_bool pybind11::detail::argument_loader<Args...>::load_impl_sequence(pybind11::detail::function_call &, pybind11::detail::index_sequence<Is...>) [with Args=<pybind11::detail::value_and_holder &, std::size_t>, Is=<0UL, 1UL>]\" \n",
            "(1980): here\n",
            "            instantiation of \"__nv_bool pybind11::detail::argument_loader<Args...>::load_args(pybind11::detail::function_call &) [with Args=<pybind11::detail::value_and_holder &, std::size_t>]\" \n",
            "/content/diffvg/pybind11/include/pybind11/pybind11.h(159): here\n",
            "            instantiation of \"void pybind11::cpp_function::initialize(Func &&, Return (*)(Args...), const Extra &...) [with Func=lambda [](pybind11::detail::value_and_holder &, std::size_t)->void, Return=void, Args=<pybind11::detail::value_and_holder &, std::size_t>, Extra=<pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>]\" \n",
            "/content/diffvg/pybind11/include/pybind11/pybind11.h(72): here\n",
            "            instantiation of \"pybind11::cpp_function::cpp_function(Func &&, const Extra &...) [with Func=lambda [](pybind11::detail::value_and_holder &, std::size_t)->void, Extra=<pybind11::name, pybind11::is_method, pybind11::sibling, pybind11::detail::is_new_style_constructor>, <unnamed>=void]\" \n",
            "/content/diffvg/pybind11/include/pybind11/pybind11.h(1162): here\n",
            "            instantiation of \"pybind11::class_<type_, options...> &pybind11::class_<type_, options...>::def(const char *, Func &&, const Extra &...) [with type_=ptr<void>, options=<>, Func=lambda [](pybind11::detail::value_and_holder &, std::size_t)->void, Extra=<pybind11::detail::is_new_style_constructor>]\" \n",
            "/content/diffvg/pybind11/include/pybind11/detail/init.h(176): here\n",
            "            instantiation of \"void pybind11::detail::initimpl::constructor<Args...>::execute(Class &, const Extra &...) [with Args=<std::size_t>, Class=pybind11::class_<ptr<void>>, Extra=<>, <unnamed>=0]\" \n",
            "/content/diffvg/pybind11/include/pybind11/pybind11.h(1191): here\n",
            "            instantiation of \"pybind11::class_<type_, options...> &pybind11::class_<type_, options...>::def(const pybind11::detail::initimpl::constructor<Args...> &, const Extra &...) [with type_=ptr<void>, options=<>, Args=<std::size_t>, Extra=<>]\" \n",
            "/content/diffvg/diffvg.cpp(1655): here\n",
            "\n",
            "[ 45%] \u001b[32m\u001b[1mLinking CXX shared library ../../../lib.linux-x86_64-3.7/libdiffvg_tf_data_ptr_no_cxx11_abi.so\u001b[0m\n",
            "[ 54%] \u001b[32m\u001b[1mLinking CXX shared library ../../../lib.linux-x86_64-3.7/libdiffvg_tf_data_ptr_cxx11_abi.so\u001b[0m\n",
            "[ 54%] Built target diffvg_tf_data_ptr_no_cxx11_abi\n",
            "[ 54%] Built target diffvg_tf_data_ptr_cxx11_abi\n",
            "In file included from /usr/local/cuda/include/thrust/detail/config/config.h:27:0,\n",
            "                 from /usr/local/cuda/include/thrust/detail/config.h:23,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:23,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/thrust/detail/config/cpp_dialect.h:104:13: warning: Thrust requires C++14. Please pass -std=c++14 to your compiler. Define THRUST_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   THRUST_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                            \n",
            "In file included from /usr/local/cuda/include/cub/util_arch.cuh:36:0,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/util.h:32,\n",
            "                 from /usr/local/cuda/include/thrust/system/cuda/detail/for_each.h:34,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/adl/for_each.h:42,\n",
            "                 from /usr/local/cuda/include/thrust/detail/for_each.inl:27,\n",
            "                 from /usr/local/cuda/include/thrust/for_each.h:279,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/transform.h:105,\n",
            "                 from /usr/local/cuda/include/thrust/detail/transform.inl:25,\n",
            "                 from /usr/local/cuda/include/thrust/transform.h:724,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.inl:23,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/generic/copy.h:58,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.inl:21,\n",
            "                 from /usr/local/cuda/include/thrust/detail/copy.h:90,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.inl:19,\n",
            "                 from /usr/local/cuda/include/thrust/system/detail/sequential/merge.h:79,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/detail/merge.h:22,\n",
            "                 from /usr/local/cuda/include/thrust/system/cpp/execution_policy.h:51,\n",
            "                 from /usr/local/cuda/include/thrust/execution_policy.h:32,\n",
            "                 from /content/diffvg/diffvg.cpp:22:\n",
            "/usr/local/cuda/include/cub/util_cpp_dialect.cuh:129:13: warning: CUB requires C++14. Please pass -std=c++14 to your compiler. Define CUB_IGNORE_DEPRECATED_CPP_DIALECT to suppress this message.\n",
            "   CUB_COMPILER_DEPRECATION(C++14, pass -std=c++14 to your compiler);\n",
            "             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~                                                                         \n",
            "\u001b[35m\u001b[1mScanning dependencies of target diffvg\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/diffvg.dir/atomic.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/diffvg.dir/parallel.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/diffvg.dir/color.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/diffvg.dir/shape.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX shared module ../lib.linux-x86_64-3.7/diffvg.so\u001b[0m\n",
            "[100%] Built target diffvg\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/render_tensorflow.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/__init__.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/image.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/color.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/shape.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/pixel_filter.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg_tensorflow/device.py -> build/bdist.linux-x86_64/egg/pydiffvg_tensorflow\n",
            "copying build/lib.linux-x86_64-3.7/libdiffvg_tf_data_ptr_cxx11_abi.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.7/diffvg.so -> build/bdist.linux-x86_64/egg\n",
            "copying build/lib.linux-x86_64-3.7/libdiffvg_tf_data_ptr_no_cxx11_abi.so -> build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/parse_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/__init__.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/save_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/image.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/color.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/shape.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/pixel_filter.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/device.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/optimize_svg.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "copying build/lib.linux-x86_64-3.7/pydiffvg/render_pytorch.py -> build/bdist.linux-x86_64/egg/pydiffvg\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/render_tensorflow.py to render_tensorflow.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/image.py to image.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/color.py to color.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/shape.py to shape.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/pixel_filter.py to pixel_filter.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg_tensorflow/device.py to device.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/parse_svg.py to parse_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/save_svg.py to save_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/image.py to image.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/color.py to color.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/shape.py to shape.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/pixel_filter.py to pixel_filter.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/device.py to device.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/optimize_svg.py to optimize_svg.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/pydiffvg/render_pytorch.py to render_pytorch.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying diffvg.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "writing build/bdist.linux-x86_64/egg/EGG-INFO/native_libs.txt\n",
            "creating dist\n",
            "creating 'dist/diffvg-0.0.1-py3.7-linux-x86_64.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "creating /usr/local/lib/python3.7/dist-packages/diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "Extracting diffvg-0.0.1-py3.7-linux-x86_64.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding diffvg 0.0.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/diffvg-0.0.1-py3.7-linux-x86_64.egg\n",
            "Processing dependencies for diffvg==0.0.1\n",
            "Searching for svgpathtools==1.4.2\n",
            "Best match: svgpathtools 1.4.2\n",
            "Adding svgpathtools 1.4.2 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for numpy==1.19.5\n",
            "Best match: numpy 1.19.5\n",
            "Adding numpy 1.19.5 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Searching for svgwrite==1.4.1\n",
            "Best match: svgwrite 1.4.1\n",
            "Adding svgwrite 1.4.1 to easy-install.pth file\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for diffvg==0.0.1\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Building wheels for collected packages: ftfy\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=92120576dcce75db5d6d28d0f74a882471dc79270c35a7360443dc48b4644f49\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built ftfy\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.0.3\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-rlxj_193\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-rlxj_193\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369076 sha256=c63e4c17fe06295fc3c6069d28a07b08fce129965c9c24fe13d3059b99c9247f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2xjlr647/wheels/fd/b9/c3/5b4470e35ed76e174bff77c92f91da82098d5e35fd5bc8cdac\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjt9T3ARukAg",
        "cellView": "form",
        "outputId": "37119d4b-0eba-4cb0-8563-ab8e94d70a6f"
      },
      "source": [
        "#@title Imports and Notebook Utilities {vertical-output: true}\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import os\n",
        "import io\n",
        "import PIL.Image, PIL.ImageDraw\n",
        "import base64\n",
        "import zipfile\n",
        "import json\n",
        "import requests\n",
        "import numpy as np\n",
        "import matplotlib.pylab as pl\n",
        "import glob\n",
        "\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "from tqdm import tqdm_notebook, tnrange\n",
        "\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "import moviepy.editor as mvp\n",
        "from moviepy.video.io.ffmpeg_writer import FFMPEG_VideoWriter\n",
        "\n",
        "\n",
        "def imread(url, max_size=None, mode=None):\n",
        "  if url.startswith(('http:', 'https:')):\n",
        "    r = requests.get(url)\n",
        "    f = io.BytesIO(r.content)\n",
        "  else:\n",
        "    f = url\n",
        "  img = PIL.Image.open(f)\n",
        "  if max_size is not None:\n",
        "    img = img.resize((max_size, max_size))\n",
        "  if mode is not None:\n",
        "    img = img.convert(mode)\n",
        "  img = np.float32(img)/255.0\n",
        "  return img\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in [np.float32, np.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "def imshow(a, fmt='jpeg'):\n",
        "  display(Image(data=imencode(a, fmt)))\n",
        "\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(len(a))))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w-len(a))%w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = len(a)//w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "from torchvision import utils\n",
        "def show_img(img):\n",
        "    img = np.transpose(img, (1, 2, 0))\n",
        "    img = np.clip(img, 0, 1)\n",
        "    img = np.uint8(img * 254)\n",
        "    # img = np.repeat(img, 4, axis=0)\n",
        "    # img = np.repeat(img, 4, axis=1)\n",
        "    pimg = PIL.Image.fromarray(img, mode=\"RGB\")\n",
        "    imshow(pimg)\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "  return img\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename='_autoplay.mp4', fps=30.0, **kw):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()\n",
        "    if self.params['filename'] == '_autoplay.mp4':\n",
        "      self.show()\n",
        "\n",
        "  def show(self, **kw):\n",
        "      self.close()\n",
        "      fn = self.params['filename']\n",
        "      display(mvp.ipython_display(fn, **kw))\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "# !pip install DALL-E"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-df506de4-5d21-a273-96de-1f9a65ffbaf2)\n",
            "Torch version: 1.9.0+cu102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z-Wt7UjTi8Le",
        "outputId": "d722630a-9232-4b59-ce5c-ca57b09545bf"
      },
      "source": [
        "#@title Load CLIP {vertical-output: true}\n",
        "\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR100\n",
        "\n",
        "# Load the model\n",
        "device = torch.device('cuda')\n",
        "model, preprocess = clip.load('ViT-B/32', device, jit=False)\n",
        "\n",
        "nouns = \"aardvark abyssinian accelerator accordion account accountant acknowledgment acoustic acrylic act action active activity actor actress adapter addition address adjustment adult advantage advertisement advice afghanistan africa aftermath afternoon aftershave afterthought age agenda agreement air airbus airmail airplane airport airship alarm albatross alcohol algebra algeria alibi alley alligator alloy almanac alphabet alto aluminium aluminum ambulance america amount amusement anatomy anethesiologist anger angle angora animal anime ankle answer ant antarctica anteater antelope anthony anthropology apartment apology apparatus apparel appeal appendix apple appliance approval april aquarius arch archaeology archeology archer architecture area argentina argument aries arithmetic arm armadillo armchair armenian army arrow art ash ashtray asia asparagus asphalt asterisk astronomy athlete atm atom attack attempt attention attic attraction august aunt australia australian author authorisation authority authorization avenue babies baboon baby back backbone bacon badge badger bag bagel bagpipe bail bait baker bakery balance balinese ball balloon bamboo banana band bandana bangladesh bangle banjo bank bankbook banker bar barbara barber barge baritone barometer base baseball basement basin basket basketball bass bassoon bat bath bathroom bathtub battery battle bay beach bead beam bean bear beard beast beat beautician beauty beaver bed bedroom bee beech beef beer beet beetle beggar beginner begonia behavior belgian belief believe bell belt bench bengal beret berry bestseller betty bibliography bicycle bike bill billboard biology biplane birch bird birth birthday bit bite black bladder blade blanket blinker blizzard block blood blouse blow blowgun blue board boat bobcat body bolt bomb bomber bone bongo bonsai book bookcase booklet boot border botany bottle bottom boundary bow bowl bowling box boy bra brace bracket brain brake branch brand brandy brass brazil bread break breakfast breath brian brick bridge british broccoli brochure broker bronze brother brother-in-law brow brown brush bubble bucket budget buffer buffet bugle building bulb bull bulldozer bumper bun burglar burma burn burst bus bush business butane butcher butter button buzzard cabbage cabinet cable cactus cafe cake calculator calculus calendar calf call camel camera camp can canada canadian cancer candle cannon canoe canvas cap capital cappelletti capricorn captain caption car caravan carbon card cardboard cardigan care carnation carol carp carpenter carriage carrot cart cartoon case cast castanet cat catamaran caterpillar cathedral catsup cattle cauliflower cause caution cave c-clamp cd ceiling celery celeste cell cellar cello celsius cement cemetery cent centimeter century ceramic cereal certification chain chair chalk chance change channel character chard charles chauffeur check cheek cheese cheetah chef chemistry cheque cherries cherry chess chest chick chicken chicory chief child children chill chime chimpanzee chin china chinese chive chocolate chord christmas christopher chronometer church cicada cinema circle circulation cirrus citizenship city clam clarinet class claus clave clef clerk click client climb clipper cloakroom clock close closet cloth cloud cloudy clover club clutch coach coal coast coat cobweb cockroach cocktail cocoa cod coffee coil coin coke cold collar college collision colombia colon colony color colt column columnist comb comfort comic comma command commission committee community company comparison competition competitor composer composition computer condition condor cone confirmation conga congo conifer connection consonant continent control cook cooking copper copy copyright cord cork cormorant corn cornet correspondent cost cotton couch cougar cough country course court cousin cover cow cowbell crab crack cracker craftsman crate crawdad crayfish crayon cream creator creature credit creditor creek crib cricket crime criminal crocodile crocus croissant crook crop cross crow crowd crown crush cry cub cuban cucumber cultivator cup cupboard cupcake curler currency current curtain curve cushion custard customer cut cuticle cycle cyclone cylinder cymbal dad daffodil dahlia daisy damage dance dancer danger daniel dash dashboard database date daughter david day dead deadline deal death deborah debt debtor decade december decimal decision decrease dedication deer defense deficit degree delete delivery den denim dentist deodorant department deposit description desert design desire desk dessert destruction detail detective development dew diamond diaphragm dibble dictionary dietician difference digestion digger digital dill dime dimple dinghy dinner dinosaur diploma dipstick direction dirt disadvantage discovery discussion disease disgust dish distance distribution distributor diving division divorced dock doctor dog dogsled doll dollar dolphin domain donald donkey donna door dorothy double doubt downtown dragon dragonfly drain drake drama draw drawbridge drawer dream dredger dress dresser dressing drill drink drive driver driving drizzle drop drug drum dry dryer duck duckling dugout dungeon dust eagle ear earth earthquake ease east edge edger editor editorial education edward eel effect egg eggnog eggplant egypt eight elbow element elephant elizabeth ellipse emery employee employer encyclopedia end enemy energy engine engineer engineering english enquiry entrance environment epoch epoxy equinox equipment era error estimate ethernet ethiopia euphonium europe evening event examination example exchange exclamation exhaust ex-husband existence expansion experience expert explanation ex-wife eye eyebrow eyelash eyeliner face facilities fact factory fahrenheit fairies fall family fan fang farm farmer fat father father-in-law faucet fear feast feather feature february fedelini feedback feeling feet felony female fender ferry ferryboat fertilizer fiber fiberglass fibre fiction field fifth fight fighter file find fine finger fir fire fired fireman fireplace firewall fish fisherman flag flame flare flat flavor flax flesh flight flock flood floor flower flugelhorn flute fly foam fog fold font food foot football footnote force forecast forehead forest forgery fork form format fortnight foundation fountain fowl fox foxglove fragrance frame france freckle freeze freezer freighter french freon friction friday fridge friend frog front frost frown fruit fuel fur furniture galley gallon game gander garage garden garlic gas gasoline gate gateway gauge gazelle gear gearshift geese gemini gender geography geology geometry george geranium german germany ghana ghost giant giraffe girdle girl gladiolus glass glider gliding glockenspiel glove glue goal goat gold goldfish golf gondola gong good-bye goose gore-tex gorilla gosling government governor grade grain gram granddaughter grandfather grandmother grandson grape graphic grass grasshopper gray grease great-grandfather great-grandmother greece greek green grenade grey grill grip ground group grouse growth guarantee guatemalan guide guilty guitar gum gun gym gymnast hacksaw hail hair haircut half-brother half-sister halibut hall hallway hamburger hammer hamster hand handball handicap handle handsaw harbor hardboard hardcover hardhat hardware harmonica harmony harp hat hate hawk head headlight headline health hearing heart heat heaven hedge height helen helicopter helium hell helmet help hemp hen heron herring hexagon hill himalayan hip hippopotamus history hobbies hockey hoe hole holiday home honey hood hook hope horn horse hose hospital hot hour hourglass house hovercraft hub hubcap humidity humor hurricane hyacinth hydrant hydrofoil hydrogen hyena hygienic ice icebreaker icicle icon idea ikebana illegal imprisonment improvement impulse inch income increase index india indonesia industry ink innocent input insect instruction instrument insulation insurance interactive interest internet interviewer intestine invention inventory invoice iran iraq iris iron island israel italian italy jacket jaguar jail jam james january japan japanese jar jasmine jason jaw jeans jeep jeff jelly jellyfish jennifer jet jewel jogging john join joke joseph journey judge judo juice july jumbo jump jumper june jury justice jute kale kamikaze kangaroo karate karen kayak kendo kenneth kenya ketchup kettle kettledrum kevin key keyboard keyboarding kick kidney kilogram kilometer kimberly kiss kitchen kite kitten kitty knee knickers knife knight knot knowledge kohlrabi korean laborer lace ladybug lake lamb lamp lan land landmine language larch lasagna latency latex lathe laugh laundry laura law lawyer layer lead leaf learning leather leek leg legal lemonade lentil leo leopard letter lettuce level libra library license lier lift light lightning lilac lily limit linda line linen link lion lip lipstick liquid liquor lisa list literature litter liver lizard llama loaf loan lobster lock locket locust look loss lotion love low lumber lunch lunchroom lung lunge lute luttuce lycra lynx lyocell lyre lyric macaroni machine macrame magazine magic magician maid mail mailbox mailman makeup malaysia male mall mallet man manager mandolin manicure manx map maple maraca marble march margaret margin maria marimba mark mark market married mary mascara mask mass match math mattock may mayonnaise meal measure meat mechanic medicine meeting melody memory men menu mercury message metal meteorology meter methane mexican mexico mice michael michelle microwave middle mile milk milkshake millennium millimeter millisecond mimosa mind mine minibus mini-skirt minister mint minute mirror missile mist mistake mitten moat modem mole mom monday money monkey month moon morning morocco mosque mosquito mother mother-in-law motion motorboat motorcycle mountain mouse moustache mouth move multi-hop multimedia muscle museum music musician mustard myanmar nail name nancy napkin narcissus nation neck need needle neon nepal nephew nerve nest net network news newsprint newsstand nic nickel niece nigeria night nitrogen node noise noodle north north america north korea norwegian nose note notebook notify novel november number numeric nurse nut nylon oak oatmeal objective oboe observation occupation ocean ocelot octagon octave october octopus odometer offence offer office oil okra olive onion open opera operation ophthalmologist opinion option orange orchestra orchid order organ organisation organization ornament ostrich otter ounce output outrigger oval oven overcoat owl owner ox oxygen oyster package packet page pail pain paint pair pajama pakistan palm pamphlet pan pancake pancreas panda pansy panther panties pantry pants panty pantyhose paper paperback parade parallelogram parcel parent parentheses park parrot parsnip part particle partner partridge party passbook passenger passive pasta paste pastor pastry patch path patient patio patricia paul payment pea peace peak peanut pear pedestrian pediatrician peen peer-to-peer pelican pen penalty pencil pendulum pentagon peony pepper perch perfume period periodical peripheral permission persian person peru pest pet pharmacist pheasant philippines philosophy phone physician piano piccolo pickle picture pie pig pigeon pike pillow pilot pimple pin pine ping pink pint pipe pisces pizza place plain plane planet plant plantation plaster plasterboard plastic plate platinum play playground playroom pleasure plier plot plough plow plywood pocket poet point poison poland police policeman polish politician pollution polo polyester pond popcorn poppy population porch porcupine port porter position possibility postage postbox pot potato poultry pound powder power precipitation preface prepared pressure price priest print printer prison probation process processing produce product production professor profit promotion propane property prose prosecution protest protocol pruner psychiatrist psychology ptarmigan puffin pull puma pump pumpkin punch punishment puppy purchase purple purpose push pvc pyjama pyramid quail quality quart quarter quartz queen question quicksand quiet quill quilt quince quit quiver quotation rabbi rabbit racing radar radiator radio radish raft rail railway rain rainbow raincoat rainstorm rake ramie random range rat rate raven ravioli ray rayon reaction reading reason receipt recess record recorder rectangle red reduction refrigerator refund regret reindeer relation relative religion relish reminder repair replace report representative request resolution respect responsibility rest restaurant result retailer revolve revolver reward rhinoceros rhythm rice richard riddle rifle ring rise risk river riverbed road roadway roast robert robin rock rocket rod roll romania romanian ronald roof room rooster root rose rotate route router rowboat rub rubber rugby rule run russia russian rutabaga ruth sack sagittarius sail sailboat sailor salad salary sale salesman salmon salt sampan samurai sand sandra sandwich santa sarah sardine satin saturday sauce saudi arabia sausage save saw saxophone scale scallion scanner scarecrow scarf scene scent schedule school science scissors scooter scorpio scorpion scraper screen screw screwdriver sea seagull seal seaplane search seashore season seat second secretary secure security seed seeder segment select selection self semicircle semicolon sense sentence separated september servant server session sex shade shadow shake shallot shame shampoo shape share shark sharon shears sheep sheet shelf shell shield shingle ship shirt shock shoe shoemaker shop shorts shoulder shovel show shrimp shrine siamese siberian side sideboard sidecar sidewalk sign signature silica silk silver sing singer single sink sister sister-in-law size skate skiing skill skin skirt sky slash slave sled sleep sleet slice slime slip slipper slope smash smell smile smoke snail snake sneeze snow snowboarding snowflake snowman snowplow snowstorm soap soccer society sociology sock soda sofa softball softdrink software soil soldier son song soprano sort sound soup sousaphone south africa south america south korea soy soybean space spade spaghetti spain spandex spark sparrow spear specialist speedboat sphere sphynx spider spike spinach spleen sponge spoon spot spring sprout spruce spy square squash squid squirrel stage staircase stamp star start starter state statement station statistic steam steel stem step step-aunt step-brother stepdaughter step-daughter step-father step-grandfather step-grandmother stepmother step-mother step-sister stepson step-son step-uncle steven stew stick stinger stitch stock stocking stomach stone stool stop stopsign stopwatch store storm story stove stranger straw stream street streetcar stretch string structure study sturgeon submarine substance subway success sudan suede sugar suggestion suit summer sun sunday sundial sunflower sunshine supermarket supply support surfboard surgeon surname surprise susan sushi swallow swamp swan sweater sweatshirt sweatshop swedish sweets swim swimming swing swiss switch sword swordfish sycamore syria syrup system table tablecloth tabletop tachometer tadpole tail tailor taiwan talk tank tanker tanzania target taste taurus tax taxi taxicab tea teacher teaching team technician teeth television teller temper temperature temple tempo tendency tennis tenor tent territory test text textbook texture thailand theater theory thermometer thing thistle thomas thought thread thrill throat throne thumb thunder thunderstorm thursday ticket tie tiger tights tile timbale time timer timpani tin tip tire titanium title toad toast toe toenail toilet tomato tom-tom ton tongue tooth toothbrush toothpaste top tornado tortellini tortoise touch tower town toy tractor trade traffic trail train tramp transaction transmission transport trapezoid tray treatment tree trial triangle trick trigonometry trip trombone trouble trousers trout trowel truck trumpet trunk t-shirt tsunami tub tuba tuesday tugboat tulip tuna tune turkey turkey turkish turn turnip turnover turret turtle tv twig twilight twine twist typhoon tyvek uganda ukraine ukrainian umbrella uncle underclothes underpants undershirt underwear unit united kingdom unshielded use utensil uzbekistan vacation vacuum valley value van vase vault vegetable vegetarian veil vein velvet venezuela venezuelan verdict vermicelli verse vessel vest veterinarian vibraphone vietnam view vinyl viola violet violin virgo viscose vise vision visitor voice volcano volleyball voyage vulture waiter waitress walk wall wallaby wallet walrus war warm wash washer wasp waste watch watchmaker water waterfall wave wax way wealth weapon weasel weather wedge wednesday weed weeder week weight whale wheel whip whiskey whistle white wholesaler whorl wilderness william willow wind windchime window windscreen windshield wine wing winter wire wish witch withdrawal witness wolf woman women wood wool woolen word work workshop worm wound wrecker wren wrench wrinkle wrist writer xylophone yacht yak yam yard yarn year yellow yew yogurt yoke yugoslavian zebra zephyr zinc zipper zone zoo zoology\"\n",
        "nouns = nouns.split(\" \")\n",
        "noun_prompts = [\"a drawing of a \" + x for x in nouns]\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    nouns_features = model.encode_text(torch.cat([clip.tokenize(noun_prompts).to(device)]))\n",
        "print(nouns_features.shape, nouns_features.dtype)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2343, 512]) torch.float16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "pP4NgiS-Jf57",
        "outputId": "a30ddee7-64be-41d7-a5e8-91c87834f949"
      },
      "source": [
        "#@title More Imports\n",
        "import pydiffvg\n",
        "import torch\n",
        "import skimage\n",
        "import skimage.io\n",
        "import random\n",
        "import ttools.modules\n",
        "import argparse\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import PIL\n",
        "from time import time\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "\n",
        "pydiffvg.set_print_timing(False)\n",
        "# Use GPU if available\n",
        "pydiffvg.set_use_gpu(torch.cuda.is_available())\n",
        "pydiffvg.set_device(device)\n",
        "\n",
        "%cd /content/diffvg/apps/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diffvg/apps\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmGSyESqOT_y"
      },
      "source": [
        "#@title Some Functions {vertical-output: true}\n",
        "def pil_resize_long_edge_to(pil, trg_size):\n",
        "  short_w = pil.width < pil.height\n",
        "  ar_resized_long = (trg_size / pil.height) if short_w else (trg_size / pil.width)\n",
        "  resized = pil.resize((int(pil.width * ar_resized_long), int(pil.height * ar_resized_long)), PIL.Image.BICUBIC)\n",
        "  return resized\n",
        "\n",
        "\n",
        "class Vgg16_Extractor(nn.Module):\n",
        "    def __init__(self, space):\n",
        "        super().__init__()\n",
        "        self.vgg_layers = models.vgg16(pretrained=True).features\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.capture_layers = [1,3,6,8,11,13,15,22,29]\n",
        "        self.space = space\n",
        "        \n",
        "    def forward_base(self, x):\n",
        "        feat = [x]\n",
        "        for i in range(len(self.vgg_layers)):\n",
        "            x = self.vgg_layers[i](x)\n",
        "            if i in self.capture_layers: feat.append(x)\n",
        "        return feat\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.space != 'vgg':\n",
        "            x = (x + 1.) / 2.\n",
        "            x = x - (torch.Tensor([0.485, 0.456, 0.406]).to(x.device).view(1, -1, 1, 1))\n",
        "            x = x / (torch.Tensor([0.229, 0.224, 0.225]).to(x.device).view(1, -1, 1, 1))\n",
        "        feat = self.forward_base(x)\n",
        "        return feat\n",
        "    \n",
        "    def forward_samples_hypercolumn(self, X, samps=100):\n",
        "        feat = self.forward(X)\n",
        "\n",
        "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
        "        xx = np.expand_dims(xx.flatten(),1)\n",
        "        xy = np.expand_dims(xy.flatten(),1)\n",
        "        xc = np.concatenate([xx,xy],1)\n",
        "        \n",
        "        samples = min(samps,xc.shape[0])\n",
        "\n",
        "        np.random.shuffle(xc)\n",
        "        xx = xc[:samples,0]\n",
        "        yy = xc[:samples,1]\n",
        "\n",
        "        feat_samples = []\n",
        "        for i in range(len(feat)):\n",
        "\n",
        "            layer_feat = feat[i]\n",
        "\n",
        "            # hack to detect lower resolution\n",
        "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
        "                xx = xx/2.0\n",
        "                yy = yy/2.0\n",
        "\n",
        "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
        "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
        "\n",
        "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
        "            feat_samples.append(features.clone().detach())\n",
        "\n",
        "        feat = torch.cat(feat_samples,1)\n",
        "        return feat\n",
        "    \n",
        "# Tensor and PIL utils\n",
        "\n",
        "def pil_loader(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        img = PIL.Image.open(f)\n",
        "        return img.convert('RGB')\n",
        "\n",
        "def pil_loader_internet(url):\n",
        "    response = requests.get(url)\n",
        "    img = PIL.Image.open(BytesIO(response.content))\n",
        "    return img.convert('RGB')\n",
        "\n",
        "def tensor_resample(tensor, dst_size, mode='bilinear'):\n",
        "    return F.interpolate(tensor, dst_size, mode=mode, align_corners=False)\n",
        "\n",
        "def pil_resize_short_edge_to(pil, trg_size):\n",
        "    short_w = pil.width < pil.height\n",
        "    ar_resized_short = (trg_size / pil.width) if short_w else (trg_size / pil.height)\n",
        "    resized = pil.resize((int(pil.width * ar_resized_short), int(pil.height * ar_resized_short)), PIL.Image.BICUBIC)\n",
        "    return resized\n",
        "\n",
        "def pil_resize_long_edge_to(pil, trg_size):\n",
        "    short_w = pil.width < pil.height\n",
        "    ar_resized_long = (trg_size / pil.height) if short_w else (trg_size / pil.width)\n",
        "    resized = pil.resize((int(pil.width * ar_resized_long), int(pil.height * ar_resized_long)), PIL.Image.BICUBIC)\n",
        "    return resized\n",
        "\n",
        "def np_to_pil(npy):\n",
        "    return PIL.Image.fromarray(npy.astype(np.uint8))\n",
        "\n",
        "def pil_to_np(pil):\n",
        "    return np.array(pil)\n",
        "\n",
        "def tensor_to_np(tensor, cut_dim_to_3=True):\n",
        "    if len(tensor.shape) == 4:\n",
        "        if cut_dim_to_3:\n",
        "            tensor = tensor[0]\n",
        "        else:\n",
        "            return tensor.data.cpu().numpy().transpose((0, 2, 3, 1))\n",
        "    return tensor.data.cpu().numpy().transpose((1,2,0))\n",
        "\n",
        "def np_to_tensor(npy, space):\n",
        "    if space == 'vgg':\n",
        "        return np_to_tensor_correct(npy)\n",
        "    return (torch.Tensor(npy.astype(np.float) / 127.5) - 1.0).permute((2,0,1)).unsqueeze(0)\n",
        "\n",
        "def np_to_tensor_correct(npy):\n",
        "    pil = np_to_pil(npy)\n",
        "    transform = transforms.Compose([transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(pil).unsqueeze(0)\n",
        "\n",
        "# Laplacian Pyramid\n",
        "\n",
        "def laplacian(x):\n",
        "    # x - upsample(downsample(x))\n",
        "    return x - tensor_resample(tensor_resample(x, [x.shape[2] // 2, x.shape[3] // 2]), [x.shape[2], x.shape[3]])\n",
        "\n",
        "def make_laplace_pyramid(x, levels):\n",
        "    pyramid = []\n",
        "    current = x\n",
        "    for i in range(levels):\n",
        "        pyramid.append(laplacian(current))\n",
        "        current = tensor_resample(current, (max(current.shape[2] // 2,1), max(current.shape[3] // 2,1)))\n",
        "    pyramid.append(current)\n",
        "    return pyramid\n",
        "\n",
        "def fold_laplace_pyramid(pyramid):\n",
        "    current = pyramid[-1]\n",
        "    for i in range(len(pyramid)-2, -1, -1): # iterate from len-2 to 0\n",
        "        up_h, up_w = pyramid[i].shape[2], pyramid[i].shape[3]\n",
        "        current = pyramid[i] + tensor_resample(current, (up_h,up_w))\n",
        "    return current\n",
        "\n",
        "def sample_indices(feat_content, feat_style):\n",
        "    indices = None\n",
        "    const = 128**2 # 32k or so\n",
        "    feat_dims = feat_style.shape[1]\n",
        "    big_size = feat_content.shape[2] * feat_content.shape[3] # num feaxels\n",
        "\n",
        "    stride_x = int(max(math.floor(math.sqrt(big_size//const)),1))\n",
        "    offset_x = np.random.randint(stride_x)\n",
        "    stride_y = int(max(math.ceil(math.sqrt(big_size//const)),1))\n",
        "    offset_y = np.random.randint(stride_y)\n",
        "    xx, xy = np.meshgrid(np.arange(feat_content.shape[2])[offset_x::stride_x], np.arange(feat_content.shape[3])[offset_y::stride_y] )\n",
        "\n",
        "    xx = xx.flatten()\n",
        "    xy = xy.flatten()\n",
        "    return xx, xy\n",
        "\n",
        "def spatial_feature_extract(feat_result, feat_content, xx, xy):\n",
        "    l2, l3 = [], []\n",
        "    device = feat_result[0].device\n",
        "\n",
        "    # for each extracted layer\n",
        "    for i in range(len(feat_result)):\n",
        "        fr = feat_result[i]\n",
        "        fc = feat_content[i]\n",
        "\n",
        "        # hack to detect reduced scale\n",
        "        if i>0 and feat_result[i-1].size(2) > feat_result[i].size(2):\n",
        "            xx = xx/2.0\n",
        "            xy = xy/2.0\n",
        "\n",
        "        # go back to ints and get residual\n",
        "        xxm = np.floor(xx).astype(np.float32)\n",
        "        xxr = xx - xxm\n",
        "\n",
        "        xym = np.floor(xy).astype(np.float32)\n",
        "        xyr = xy - xym\n",
        "\n",
        "        # do bilinear resample\n",
        "        w00 = torch.from_numpy((1.-xxr)*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
        "        w01 = torch.from_numpy((1.-xxr)*xyr).float().view(1, 1, -1, 1).to(device)\n",
        "        w10 = torch.from_numpy(xxr*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
        "        w11 = torch.from_numpy(xxr*xyr).float().view(1, 1, -1, 1).to(device)\n",
        "\n",
        "        xxm = np.clip(xxm.astype(np.int32),0,fr.size(2)-1)\n",
        "        xym = np.clip(xym.astype(np.int32),0,fr.size(3)-1)\n",
        "\n",
        "        s00 = xxm*fr.size(3)+xym\n",
        "        s01 = xxm*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
        "        s10 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+(xym)\n",
        "        s11 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
        "\n",
        "        fr = fr.view(1,fr.size(1),fr.size(2)*fr.size(3),1)\n",
        "        fr = fr[:,:,s00,:].mul_(w00).add_(fr[:,:,s01,:].mul_(w01)).add_(fr[:,:,s10,:].mul_(w10)).add_(fr[:,:,s11,:].mul_(w11))\n",
        "\n",
        "        fc = fc.view(1,fc.size(1),fc.size(2)*fc.size(3),1)\n",
        "        fc = fc[:,:,s00,:].mul_(w00).add_(fc[:,:,s01,:].mul_(w01)).add_(fc[:,:,s10,:].mul_(w10)).add_(fc[:,:,s11,:].mul_(w11))\n",
        "\n",
        "        l2.append(fr)\n",
        "        l3.append(fc)\n",
        "\n",
        "    x_st = torch.cat([li.contiguous() for li in l2],1)\n",
        "    c_st = torch.cat([li.contiguous() for li in l3],1)\n",
        "\n",
        "    xx = torch.from_numpy(xx).view(1,1,x_st.size(2),1).float().to(device)\n",
        "    yy = torch.from_numpy(xy).view(1,1,x_st.size(2),1).float().to(device)\n",
        "    \n",
        "    x_st = torch.cat([x_st,xx,yy],1)\n",
        "    c_st = torch.cat([c_st,xx,yy],1)\n",
        "    return x_st, c_st\n",
        "\n",
        "def pairwise_distances_cos(x, y):\n",
        "    x_norm = torch.sqrt((x**2).sum(1).view(-1, 1))\n",
        "    y_t = torch.transpose(y, 0, 1)\n",
        "    y_norm = torch.sqrt((y**2).sum(1).view(1, -1))\n",
        "    dist = 1.-torch.mm(x, y_t)/x_norm/y_norm\n",
        "    return dist\n",
        "\n",
        "def pairwise_distances_sq_l2(x, y):\n",
        "    x_norm = (x**2).sum(1).view(-1, 1)\n",
        "    y_t = torch.transpose(y, 0, 1)\n",
        "    y_norm = (y**2).sum(1).view(1, -1)\n",
        "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
        "    return torch.clamp(dist, 1e-5, 1e5)/x.size(1)\n",
        "\n",
        "def distmat(x, y, cos_d=True):\n",
        "    if cos_d:\n",
        "        M = pairwise_distances_cos(x, y)\n",
        "    else:\n",
        "        M = torch.sqrt(pairwise_distances_sq_l2(x, y))\n",
        "    return M\n",
        "\n",
        "def content_loss(feat_result, feat_content):\n",
        "    d = feat_result.size(1)\n",
        "\n",
        "    X = feat_result.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
        "    Y = feat_content.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
        "\n",
        "    Y = Y[:,:-2]\n",
        "    X = X[:,:-2]\n",
        "    # X = X.t()\n",
        "    # Y = Y.t()\n",
        "\n",
        "    Mx = distmat(X, X)\n",
        "    Mx = Mx#/Mx.sum(0, keepdim=True)\n",
        "\n",
        "    My = distmat(Y, Y)\n",
        "    My = My#/My.sum(0, keepdim=True)\n",
        "\n",
        "    d = torch.abs(Mx-My).mean()# * X.shape[0]\n",
        "    return d\n",
        "\n",
        "def rgb_to_yuv(rgb):\n",
        "    C = torch.Tensor([[0.577350,0.577350,0.577350],[-0.577350,0.788675,-0.211325],[-0.577350,-0.211325,0.788675]]).to(rgb.device)\n",
        "    yuv = torch.mm(C,rgb)\n",
        "    return yuv\n",
        "\n",
        "def style_loss(X, Y, cos_d=True):\n",
        "    d = X.shape[1]\n",
        "\n",
        "    if d == 3:\n",
        "        X = rgb_to_yuv(X.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
        "        Y = rgb_to_yuv(Y.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
        "    else:\n",
        "        X = X.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
        "        Y = Y.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
        "\n",
        "    # Relaxed EMD\n",
        "    CX_M = distmat(X, Y, cos_d=True)\n",
        "\n",
        "    if d==3: CX_M = CX_M + distmat(X, Y, cos_d=False)\n",
        "\n",
        "    m1, m1_inds = CX_M.min(1)\n",
        "    m2, m2_inds = CX_M.min(0)\n",
        "\n",
        "    remd = torch.max(m1.mean(), m2.mean())\n",
        "\n",
        "    return remd\n",
        "\n",
        "def moment_loss(X, Y, moments=[1,2]):\n",
        "    loss = 0.\n",
        "    X = X.squeeze().t()\n",
        "    Y = Y.squeeze().t()\n",
        "\n",
        "    mu_x = torch.mean(X, 0, keepdim=True)\n",
        "    mu_y = torch.mean(Y, 0, keepdim=True)\n",
        "    mu_d = torch.abs(mu_x - mu_y).mean()\n",
        "\n",
        "    if 1 in moments:\n",
        "        # print(mu_x.shape)\n",
        "        loss = loss + mu_d\n",
        "\n",
        "    if 2 in moments:\n",
        "        X_c = X - mu_x\n",
        "        Y_c = Y - mu_y\n",
        "        X_cov = torch.mm(X_c.t(), X_c) / (X.shape[0] - 1)\n",
        "        Y_cov = torch.mm(Y_c.t(), Y_c) / (Y.shape[0] - 1)\n",
        "\n",
        "        # print(X_cov.shape)\n",
        "        # exit(1)\n",
        "\n",
        "        D_cov = torch.abs(X_cov - Y_cov).mean()\n",
        "        loss = loss + D_cov\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calculate_loss(feat_result, feat_content, feat_style, indices, content_weight, moment_weight=1.0):\n",
        "  # spatial feature extract\n",
        "  num_locations = 1024\n",
        "  spatial_result, spatial_content = spatial_feature_extract(feat_result, feat_content, indices[0][:num_locations], indices[1][:num_locations])\n",
        "  # loss_content = content_loss(spatial_result, spatial_content)\n",
        "\n",
        "  d = feat_style.shape[1]\n",
        "  spatial_style = feat_style.view(1, d, -1, 1)\n",
        "  feat_max = 3+2*64+128*2+256*3+512*2 # (sum of all extracted channels)\n",
        "\n",
        "  loss_remd = style_loss(spatial_result[:, :feat_max, :, :], spatial_style[:, :feat_max, :, :])\n",
        "\n",
        "  loss_moment = moment_loss(spatial_result[:,:-2,:,:], spatial_style, moments=[1,2]) # -2 is so that it can fit?\n",
        "  # palette matching\n",
        "  content_weight_frac = 1./max(content_weight,1.)\n",
        "  loss_moment += content_weight_frac * style_loss(spatial_result[:,:3,:,:], spatial_style[:,:3,:,:])\n",
        "  \n",
        "  loss_style = loss_remd + moment_weight * loss_moment\n",
        "  # print(f'Style: {loss_style.item():.3f}, Content: {loss_content.item():.3f}')\n",
        "\n",
        "  style_weight = 1.0 + moment_weight\n",
        "  loss_total = (loss_style) / (content_weight + style_weight)\n",
        "  return loss_total\n",
        "\n",
        "def get_image_augmentation(use_normalized_clip):\n",
        "    augment_trans = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
        "    ])\n",
        "\n",
        "    if use_normalized_clip:\n",
        "        augment_trans = transforms.Compose([\n",
        "        transforms.RandomPerspective(fill=1, p=1, distortion_scale=0.5),\n",
        "        transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
        "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "    ])\n",
        "    return augment_trans\n",
        "\n",
        "def initialize_curves(num_paths, canvas_width, canvas_height):\n",
        "    shapes = []\n",
        "    shape_groups = []\n",
        "    for i in range(num_paths):\n",
        "        num_segments = random.randint(1, 3)\n",
        "        num_control_points = torch.zeros(num_segments, dtype = torch.int32) + 2\n",
        "        points = []\n",
        "        p0 = (random.random(), random.random())\n",
        "        points.append(p0)\n",
        "        for j in range(num_segments):\n",
        "            radius = 0.1\n",
        "            p1 = (p0[0] + radius * (random.random() - 0.5), p0[1] + radius * (random.random() - 0.5))\n",
        "            p2 = (p1[0] + radius * (random.random() - 0.5), p1[1] + radius * (random.random() - 0.5))\n",
        "            p3 = (p2[0] + radius * (random.random() - 0.5), p2[1] + radius * (random.random() - 0.5))\n",
        "            points.append(p1)\n",
        "            points.append(p2)\n",
        "            points.append(p3)\n",
        "            p0 = p3\n",
        "        points = torch.tensor(points)\n",
        "        points[:, 0] *= canvas_width\n",
        "        points[:, 1] *= canvas_height\n",
        "        path = pydiffvg.Path(num_control_points = num_control_points, points = points, stroke_width = torch.tensor(1.0), is_closed = False)\n",
        "        shapes.append(path)\n",
        "        path_group = pydiffvg.ShapeGroup(shape_ids = torch.tensor([len(shapes) - 1]), fill_color = None, stroke_color = torch.tensor([random.random(), random.random(), random.random(), random.random()]))\n",
        "        shape_groups.append(path_group)\n",
        "    return shapes, shape_groups\n",
        "\n",
        "def render_drawing(shapes, shape_groups,\\\n",
        "                   canvas_width, canvas_height, n_iter, save=False):\n",
        "    scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n",
        "        canvas_width, canvas_height, shapes, shape_groups)\n",
        "    render = pydiffvg.RenderFunction.apply\n",
        "    img = render(canvas_width, canvas_height, 2, 2, n_iter, None, *scene_args)\n",
        "    img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])        \n",
        "    if save:\n",
        "        pydiffvg.imwrite(img.cpu(), '/content/res/iter_{}.png'.format(int(n_iter)), gamma=1.0)\n",
        "    img = img[:, :, :3]\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
        "    return img"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XIVMSJuWgxG"
      },
      "source": [
        "#@title Curve Optimizer {vertical-output: true}\n",
        "\n",
        "def style_clip_draw(prompt, style_path, \\\n",
        "                    num_paths=256, num_iter=1000, max_width=50,\\\n",
        "                    num_augs=4, style_opt_freq=5, style_opt_iter=50,\n",
        "                    neg_prompt=None, neg_prompt_2=None,\\\n",
        "                    use_normalized_clip=False):\n",
        "    '''\n",
        "    Perform StyleCLIPDraw using a given text prompt and style image\n",
        "    args:\n",
        "        prompt (str) : Text prompt to draw\n",
        "        style_path(str) : Style image path or url\n",
        "    kwargs:\n",
        "        num_paths (int) : Number of brush strokes\n",
        "        num_iter(int) : Number of optimization iterations\n",
        "        max_width(float) : Maximum width of a brush stroke in pixels\n",
        "        num_augs(int) : Number of image augmentations\n",
        "        style_opt_freq(int) : How often to do style optimization. Low value is high frequency\n",
        "        style_opt_iter(int) : How many iterations to do in the style optimization loop\n",
        "        neg_prompt(str) : Negative prompt. None if you don't want it\n",
        "        neg_prompt_2(str) : Negative prompt. None if you don't want it\n",
        "        use_normalized_clip(bool)\n",
        "    '''\n",
        "    text_input = clip.tokenize(prompt).to(device)\n",
        "\n",
        "    if neg_prompt is not None: text_input_neg1 = clip.tokenize(neg_prompt).to(device)\n",
        "    if neg_prompt_2 is not None: text_input_neg2 = clip.tokenize(neg_prompt_2).to(device)\n",
        "\n",
        "    # Calculate features\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_input)\n",
        "        if neg_prompt is not None: text_features_neg1 = model.encode_text(text_input_neg1)\n",
        "        if neg_prompt_2 is not None: text_features_neg2 = model.encode_text(text_input_neg2)\n",
        "\n",
        "    canvas_width, canvas_height = 224*2, 224\n",
        "\n",
        "    # Image Augmentation Transformation\n",
        "    augment_trans = get_image_augmentation(use_normalized_clip)\n",
        "\n",
        "    # Initialize Random Curves\n",
        "    shapes, shape_groups = initialize_curves(num_paths, canvas_width, canvas_height)\n",
        "\n",
        "    # Just some diffvg setup\n",
        "    # scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n",
        "    #     canvas_width, canvas_height, shapes, shape_groups)\n",
        "    # render = pydiffvg.RenderFunction.apply\n",
        "    # img = render(canvas_width, canvas_height, 2, 2, 0, None, *scene_args)\n",
        "\n",
        "    points_vars = []\n",
        "    stroke_width_vars = []\n",
        "    color_vars = []\n",
        "    for path in shapes:\n",
        "        path.points.requires_grad = True\n",
        "        points_vars.append(path.points)\n",
        "        path.stroke_width.requires_grad = True\n",
        "        stroke_width_vars.append(path.stroke_width)\n",
        "    for group in shape_groups:\n",
        "        group.stroke_color.requires_grad = True\n",
        "        color_vars.append(group.stroke_color)\n",
        "\n",
        "    # Optimizers\n",
        "    points_optim = torch.optim.Adam(points_vars, lr=1.0)\n",
        "    width_optim = torch.optim.Adam(stroke_width_vars, lr=0.1)\n",
        "    color_optim = torch.optim.Adam(color_vars, lr=0.01)\n",
        "\n",
        "    # points_vars = [l.data.requires_grad_() for l in points_vars]\n",
        "    points_optim_style = torch.optim.RMSprop(points_vars, lr=0.1)\n",
        "    width_optim_style = torch.optim.RMSprop(stroke_width_vars, lr=0.1)\n",
        "    color_optim_style = torch.optim.RMSprop(color_vars, lr=0.01)\n",
        "\n",
        "    style_pil = pil_loader(style_path) if os.path.exists(style_path) else pil_loader_internet(style_path)\n",
        "    style_pil = pil_resize_long_edge_to(style_pil, canvas_width)\n",
        "    style_np = pil_to_np(style_pil)\n",
        "    style = np_to_tensor(style_np, \"normal\").to(device)\n",
        "    extractor = Vgg16_Extractor(space=\"normal\").to(device)\n",
        "\n",
        "    # Extract style features from style image\n",
        "    feat_style = None\n",
        "    for i in range(5):\n",
        "        with torch.no_grad():\n",
        "        # r is region of interest (mask)\n",
        "            feat_e = extractor.forward_samples_hypercolumn(style, samps=1000)\n",
        "            feat_style = feat_e if feat_style is None else torch.cat((feat_style, feat_e), dim=2)\n",
        "\n",
        "    # Run the main optimization loop\n",
        "    for t in range(num_iter):\n",
        "\n",
        "        # Anneal learning rate (makes videos look cleaner)\n",
        "        if t == int(num_iter * 0.5):\n",
        "            for g in points_optim.param_groups:\n",
        "                g['lr'] = 0.4\n",
        "        if t == int(num_iter * 0.75):\n",
        "            for g in points_optim.param_groups:\n",
        "                g['lr'] = 0.1\n",
        "        \n",
        "        points_optim.zero_grad()\n",
        "        width_optim.zero_grad()\n",
        "        color_optim.zero_grad()\n",
        "\n",
        "        img = render_drawing(shapes, shape_groups, canvas_width, canvas_height, t, save=(t % 5 == 0))\n",
        "        \n",
        "        loss = 0\n",
        "        img_augs = []\n",
        "        for n in range(num_augs):\n",
        "            img_augs.append(augment_trans(img))\n",
        "        im_batch = torch.cat(img_augs)\n",
        "        image_features = model.encode_image(im_batch)\n",
        "        for n in range(num_augs):\n",
        "            loss -= torch.cosine_similarity(text_features, image_features[n:n+1], dim=1)\n",
        "            if neg_prompt is not None: loss += torch.cosine_similarity(text_features_neg1, image_features[n:n+1], dim=1) * 0.3\n",
        "            if neg_prompt_2 is not None: loss += torch.cosine_similarity(text_features_neg2, image_features[n:n+1], dim=1) * 0.3\n",
        "\n",
        "        loss.backward()\n",
        "        points_optim.step()\n",
        "        width_optim.step()\n",
        "        color_optim.step()\n",
        "        \n",
        "        # Do style optimization from time to time\n",
        "        if t%style_opt_freq == 0:\n",
        "          img = render_drawing(shapes, shape_groups, canvas_width, canvas_height, t)\n",
        "          feat_content = extractor(img)\n",
        "\n",
        "          xx, xy = sample_indices(feat_content[0], feat_style) # 0 to sample over first layer extracted\n",
        "          for it in range(style_opt_iter):\n",
        "              styleloss=0\n",
        "              points_optim_style.zero_grad()\n",
        "              width_optim_style.zero_grad()\n",
        "              color_optim_style.zero_grad()\n",
        "\n",
        "              img = render_drawing(shapes, shape_groups, canvas_width, canvas_height, t)\n",
        "              feat_content = extractor(img) \n",
        "\n",
        "              if it % 1 == 0 and it != 0:\n",
        "                  np.random.shuffle(xx)\n",
        "                  np.random.shuffle(xy)\n",
        "\n",
        "              styleloss = calculate_loss(feat_content, feat_content, feat_style, [xx, xy], 0)\n",
        "\n",
        "              styleloss.backward()\n",
        "              points_optim_style.step()\n",
        "              width_optim_style.step()\n",
        "              color_optim_style.step()\n",
        "\n",
        "        for path in shapes:\n",
        "            path.stroke_width.data.clamp_(1.0, max_width)\n",
        "        for group in shape_groups:\n",
        "            group.stroke_color.data.clamp_(0.0, 1.0)\n",
        "        \n",
        "        if t % 5 == 0:\n",
        "            show_img(img.detach().cpu().numpy()[0])\n",
        "            # show_img(torch.cat([img.detach(), img_aug.detach()], axis=3).cpu().numpy()[0])\n",
        "            print('render loss:', loss.item())\n",
        "            print('iteration:', t)\n",
        "            with torch.no_grad():\n",
        "                im_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "                noun_norm = nouns_features / nouns_features.norm(dim=-1, keepdim=True)\n",
        "                similarity = (100.0 * im_norm @ noun_norm.T).softmax(dim=-1)\n",
        "                values, indices = similarity[0].topk(5)\n",
        "                print(\"\\nTop predictions:\\n\")\n",
        "                for value, index in zip(values, indices):\n",
        "                    print(f\"{nouns[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "    return render_drawing(shapes, shape_groups, canvas_width, canvas_height, t).detach().cpu().numpy()[0]\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic18gny1aXAt"
      },
      "source": [
        "# Loop through a few prompts and do style draw with a few different styles.  Then save the resulting images.\n",
        "style_paths = ['https://m.media-amazon.com/images/I/717uYtyEK6L._SL1000_.jpg',\n",
        "               'https://img.freepik.com/free-vector/woman-minimal-hand-drawn-illustration-one-line-style-drawing_202497-238.jpg?size=338&ext=jpg',\n",
        "               'https://a2.cdn.japantravel.com/photo/66265-221559/1440x960!/tokyo-mondrian-exhibition-221559.jpg',\n",
        "               ]\n",
        "prompts = ['A fish is talking on a cell phone in a busy city street.', 'Barack Obama', 'A cat playing piano.', 'Two birds watching TV.']\n",
        "for prompt in prompts:\n",
        "  i = 0\n",
        "  for style_path in style_paths:\n",
        "    img = style_clip_draw(prompt, style_path,\\\n",
        "                          num_iter=500, style_opt_freq=5, style_opt_iter=50)\n",
        "    im = PIL.Image.fromarray((img*255).astype(np.uint8).transpose((1,2,0)))\n",
        "    im.save(\"/\" + prompt + str(i) + \".jpeg\")\n",
        "    i += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eru5XUuOCi6c",
        "cellView": "form"
      },
      "source": [
        "#@title Video Renderer {vertical-output: true}\n",
        "\n",
        "# Render a picture with each stroke.\n",
        "with torch.no_grad():\n",
        "    for i in range(args.num_paths):\n",
        "        print(i)\n",
        "        scene_args = pydiffvg.RenderFunction.serialize_scene(\\\n",
        "            canvas_width, canvas_height, shapes[:i+1], shape_groups[:i+1])\n",
        "        img = render(canvas_width, canvas_height, 2, 2, t, None, *scene_args)\n",
        "        img = img[:, :, 3:4] * img[:, :, :3] + torch.ones(img.shape[0], img.shape[1], 3, device = pydiffvg.get_device()) * (1 - img[:, :, 3:4])\n",
        "        pydiffvg.imwrite(img.cpu(), '/content/res/stroke_{}.png'.format(i), gamma=gamma)\n",
        "print(\"ffmpeging\")\n",
        "\n",
        "# Convert the intermediate renderings to a video.\n",
        "from subprocess import call\n",
        "call([\"ffmpeg\", \"-y\", \"-framerate\", \"60\", \"-i\",\n",
        "    \"/content/res/iter_%d.png\", \"-vb\", \"20M\",\n",
        "    \"/content/res/out.mp4\"])\n",
        "\n",
        "call([\"ffmpeg\", \"-y\", \"-framerate\", \"60\", \"-i\",\n",
        "    \"/content/res/stroke_%d.png\", \"-vb\", \"20M\",\n",
        "    \"/content/res/out_strokes.mp4\"])\n",
        "\n",
        "call([\"ffmpeg\", \"-y\", \"-i\", \"/content/res/out.mp4\", \"-filter_complex\",\n",
        "    \"[0]trim=0:2[hold];[0][hold]concat[extended];[extended][0]overlay\",\n",
        "    \"/content/res/out_longer.mp4\"])\n",
        "\n",
        "call([\"ffmpeg\", \"-y\", \"-i\", \"/content/res/out_strokes.mp4\", \"-filter_complex\",\n",
        "    \"[0]trim=0:2[hold];[0][hold]concat[extended];[extended][0]overlay\",\n",
        "    \"/content/res/out_strokes_longer.mp4\"])\n",
        "\n",
        "\n",
        "display(mvp.ipython_display(\"/content/res/out_longer.mp4\"))\n",
        "display(mvp.ipython_display(\"/content/res/out_strokes_longer.mp4\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2jqNT0VYPWp",
        "cellView": "form"
      },
      "source": [
        "#@title Pixel Optimizer (Ignore) {vertical-output: true}\n",
        "\n",
        "%cd /content/diffvg/apps/\n",
        "\n",
        "prompt = \"Underwater\"\n",
        "text_input = clip.tokenize(prompt).to(device)\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text_input)\n",
        "\n",
        "import torch\n",
        "import skimage\n",
        "import skimage.io\n",
        "import random\n",
        "import ttools.modules\n",
        "import argparse\n",
        "import math\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "class ImageBase(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.p = torch.nn.Parameter(torch.ones(224, 224, 3))\n",
        "    def forward(self):\n",
        "        return torch.nn.functional.sigmoid(self.p)\n",
        "\n",
        "device = torch.device('cuda')\n",
        "canvas_width, canvas_height = 224, 224\n",
        "\n",
        "augment_trans = transforms.Compose([\n",
        "    transforms.RandomPerspective(fill=1, p=1),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7,0.9)),\n",
        "])\n",
        "\n",
        "ib = ImageBase().to(device)\n",
        "\n",
        "t_img = imread('https://lh5.googleusercontent.com/mjvIYutjtOGEEU2cBYuFMvCrBCg4-MGh3DqCRlLqwn5I6VvdKdtwWvAYlndQbv-VUudPcecQ_TEGFjYaTuS_r0LNI83Sp8MlXJb6OarJ9mu-IkmKPlg9Gaw3gOjQvvgvuUB5ghJjlaE')\n",
        "target = torch.from_numpy(t_img).to(torch.float32)\n",
        "ib.p = target\n",
        "\n",
        "# Optimize\n",
        "optim = torch.optim.Adam(ib.parameters(), lr=0.01)\n",
        "# Adam iterations.\n",
        "for t in range(args.num_iter):\n",
        "    optim.zero_grad()\n",
        "    img = ib()\n",
        "    # Convert img from HWC to NCHW\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.permute(0, 3, 1, 2) # NHWC -> NCHW\n",
        "\n",
        "    loss = 0\n",
        "    for n in range(16):\n",
        "        img_aug = augment_trans(img)\n",
        "        image_features = model.encode_image(img_aug)\n",
        "        loss -= torch.cosine_similarity(text_features, image_features, dim=1)\n",
        "        # loss += torch.abs(torch.mean(1-img_aug)) * 0.1\n",
        "\n",
        "    # Backpropagate the gradients.\n",
        "    loss.backward()\n",
        "\n",
        "    # Take a gradient descent step.\n",
        "    optim.step()\n",
        "    \n",
        "    if t % 10 == 0:\n",
        "        show_img(img.detach().cpu().numpy()[0])\n",
        "        show_img(img_aug.detach().cpu().numpy()[0])\n",
        "        print('render loss:', loss.item())\n",
        "        print('iteration:', t)\n",
        "        with torch.no_grad():\n",
        "            im_norm = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "            noun_norm = nouns_features / nouns_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = (100.0 * im_norm @ noun_norm.T).softmax(dim=-1)\n",
        "            values, indices = similarity[0].topk(5)\n",
        "            print(\"\\nTop predictions:\\n\")\n",
        "            for value, index in zip(values, indices):\n",
        "                print(f\"{nouns[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "\n",
        "# Convert the intermediate renderings to a video.\n",
        "from subprocess import call\n",
        "call([\"ffmpeg\", \"-framerate\", \"24\", \"-i\",\n",
        "    \"results/painterly_rendering/iter_%d.png\", \"-vb\", \"20M\",\n",
        "    \"results/painterly_rendering/out.mp4\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}